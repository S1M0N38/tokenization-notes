# Tokenization Notes

This is a simple notebook with some notes about tokenization in LLM.
It was used to introduce how BPE tokenization works and its limitations.

Byte Latent Transformer is an effort by Meta to replace BPE tokenization with trainable layers which operate at character level.

### Resources

These are a MUST:

- *"Let's build the GPT Tokenizer"* by Andrej Karpathy [[video](https://www.youtube.com/watch?v=8-l0-9s6-8I)]
- *"Byte Latent Transformer: Patches Scale Better Than Tokens"* by Meta [[paper](https://arxiv.org/abs/2412.09871)]

Blog posts from Vizuara:

- *"The necessary (and neglected) evil of Large Language Models: Tokenization"* [[blog post](https://vizuara.substack.com/p/the-necessary-and-neglected-evil)]
- *"Why unicode or character tokenization fails?"* [[blog post](https://vizuara.substack.com/p/why-unicode-or-character-tokenization)]
- *"Understanding Byte Pair Encoding (BPE) in Large Language Models"* [[blog post](https://vizuara.substack.com/p/understanding-byte-pair-encoding)]
- *"Byte Latent Transformers : Patches Scale Better Than Tokens"* [[blog post](https://vizuara.substack.com/p/byte-latent-transformers-patches)]
- *"Large Concept models : Language Modeling in a Sentence Representation Space"* [[blog post](https://vizuara.substack.com/p/large-concept-models-language-modeling)]

Suggested further reading:

- *"Large Concept Models: Language Modeling in a Sentence Representation Space"* [[paper](https://arxiv.org/abs/2412.08821)]
