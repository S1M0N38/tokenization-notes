{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3015dd63",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import regex as re\n",
    "from tiktoken._educational import *\n",
    "\n",
    "# Let's experiment with gpt2 tokenizer\n",
    "tokenizer = SimpleBytePairEncoding.from_tiktoken(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b830432",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `rm -rf tokenization` ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cddaaf1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text Tokenization 101\n",
    "\n",
    "It's a preprocessing step separated from the actual LLM.\n",
    "\n",
    "It translates human-friendly format (text) to machine-friendly format (numbers).\n",
    "\n",
    "At its core, the tokenizer is an object with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f62912e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encodes a string into tokens.\n",
      "\n",
      "        >>> enc.encode(\"hello world\")\n",
      "        [388, 372]\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b667e97d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decodes a list of tokens into a string.\n",
      "\n",
      "        Decoded bytes are not guaranteed to be valid UTF-8. In that case, we replace\n",
      "        the invalid bytes with the replacement character \"ÔøΩ\".\n",
      "\n",
      "        >>> enc.decode([388, 372])\n",
      "        'hello world'\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8767ce6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabolary size: 50256\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabolary size:\", len(tokenizer.mergeable_ranks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566fd5c6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In LLM, the `list[int]` returned by `tokenizer.encode` are the **row indices of Embedding Matrix `wte_in`** (weights for input text embedding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a46abe14",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_in='Hello World üëã'\n",
      "toks_in=[15496, 2159, 50169, 233]\n",
      "\n",
      "... LLM computations ...\n",
      "\n",
      "toks_out=[17250, 30325, 232]\n",
      "text_out='Hi üòä'\n"
     ]
    }
   ],
   "source": [
    "                                   # T (time) current context length\n",
    "V = len(tokenizer.mergeable_ranks) # V vocabolary size\n",
    "C = 1024                           # C (channel) embedding of size\n",
    "\n",
    "text_in = \"Hello World üëã\"\n",
    "toks_in = tokenizer.encode(text_in, None)  # (T,)\n",
    "print(f\"{text_in=}\\n{toks_in=}\")\n",
    "\n",
    "wte_in = np.random.random((V, C)) # [trained]\n",
    "emb_in = wte_in[toks_in]          # (T, C)\n",
    "\n",
    "# emb_in -> transformer blocks [trained] -> toks_out\n",
    "print(\"\\n... LLM computations ...\\n\")\n",
    "\n",
    "toks_out = [17250, 30325, 232] # results from LLM\n",
    "text_out = tokenizer.decode(toks_out)\n",
    "print(f\"{toks_out=}\\n{text_out=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2699d0b0",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### LLM Inference 101\n",
    "\n",
    "```python\n",
    "                                       # T (time) current context length\n",
    "V = len(tokenizer.mergeable_ranks) + 1 # V vocabolary size\n",
    "C = 1024                               # C (channel) embedding of size\n",
    "\n",
    "toks_in = [15496, 2159, 50169, 233]\n",
    "toks_out = []\n",
    "toks = toks_in + toks_out \n",
    "\n",
    "wte_in = np.random.random((V, C))     # [trained]\n",
    "wte_out = np.random.random((V, C))    # [trained]\n",
    "\n",
    "def transofrmers(emb_in: np.array) -> np.array: ... # [trained]\n",
    "\n",
    "while tok_out != 50256: # not \"<|endoftext|>\"\n",
    "    \n",
    "    emb_in = wte_in[toks_in]           # (T, C)\n",
    "    emb_out = transofrmers(emb_in)     # (T, C)\n",
    "    logits = wte_out @ emb_out[[-1]].T # (V, C) @ (C, 1) -> (V, 1)\n",
    "    tok_out = logits.argmax()          # simple sample\n",
    "    \n",
    "    toks.append(tok_out)\n",
    "    toks_out.append(tok_out)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceabea4f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Strings to Integers (Naive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29beb18a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Strings in Python are immutable sequences of **Unicode code points**.\n",
    "- Unicode points map **a character to an integer**.\n",
    "- The built-in `ord` function returns the Unicode point of a character (a string of length 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d4af33a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[72, 101, 108, 108, 111, 32, 87, 111, 114, 108, 100, 32, 128075]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ord(c) for c in \"Hello World üëã\"] \n",
    "\n",
    "# Unicode code points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aba965",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Unicode defines itself 3 encodings: **UTF-8**, UTF-16, and UTF-32.\n",
    "- Encodings define how to convert Unicode text (Python strings) to bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00e8a7fe",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[72, 101, 108, 108, 111, 32, 87, 111, 114, 108, 100, 32, 240, 159, 145, 139]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(\"Hello World üëã\".encode(\"UTF-8\")) \n",
    "\n",
    "# Bytes (base 10) of UTF-8 encoded string "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cffd14a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why not Unicode code points?\n",
    "\n",
    "- The Unicode standard is actively developed (variable vocabulary).\n",
    "- The number of Unicode code points (~154k) ‚áí V is big.\n",
    "- This results in a long list of tokens ‚áí T is big.\n",
    "- A single code point does not carry semantic information.\n",
    "- Rare code points will correspond to undertrained embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71c0e5e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Why not Raw Bytes?\n",
    "\n",
    "- Small and fixed vocabulary ‚áí V = 256.\n",
    "- This results in a really long list of tokens ‚áí T is very big.\n",
    "- A single byte does not carry semantic information.\n",
    "- Not all byte streams correspond to valid Unicode text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef0c68f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "---\n",
    "\n",
    "However, starting from raw bytes, we can use BPE to:\n",
    "\n",
    "- cleverly extend the vocabulary at our will ‚áí V = 256 + const.\n",
    "- drastically reduce encoding length T.\n",
    "- have tokens carrying semantic information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e155e03",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Byte Pair Encoding (BPE)\n",
    "\n",
    "- **‚Üí compress/encode**: from original source to compressed version\n",
    "- **‚Üê decompress/decode**: from compressed version to original version\n",
    "\n",
    "---\n",
    "```md\n",
    "aaabdaaabac ‚áÑ   ZabdZabac   ‚áÑ    ZYdZYac    ‚áÑ    XdXac     [symbols]\n",
    "\n",
    "    { }       { \"Z\": \"aa\" }   { \"Z\": \"aa\",    { \"Z\": \"aa\", [merges]\n",
    "                                \"Y\": \"ab\" }     \"Y\": \"ab\",\n",
    "                                                \"X\": \"ZY\" }\n",
    "```\n",
    "---\n",
    "\n",
    "- *vocabulary size* = num. of raw symbols + num. of merges.\n",
    "- *\"train\"* = construct merges dictionary by running BPE over text corpus.\n",
    "- *\"inference\"* = make use of merges dictonary to encode/decode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f246cbfd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Regex Pattern\n",
    "\n",
    "Simply converting text to bytes and then applying BPE could produce different tokens for: `dog`, `dog.`, `dog!`, and `dog?`.\n",
    "\n",
    "Therefore, the first step is to disentangle semantic and syntactic elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c911bfe5",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ' World', ' üëã']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = re.compile(\n",
    "    r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| \"\"\"\n",
    "    r\"\"\"?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    ")\n",
    "re.findall(pattern, \"Hello World üëã\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3ca814",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Tokenization example\n",
    "\n",
    "Explore various tokenizers interactively on [tiktokenizer](https://tiktokenizer.vercel.app/?model=gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25fa08d5",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[48;5;167mH\u001b[48;5;179me\u001b[48;5;185ml\u001b[48;5;77ml\u001b[48;5;80mo\u001b[0m\n",
      "\u001b[48;5;167mH\u001b[48;5;179me\u001b[48;5;185mll\u001b[48;5;80mo\u001b[0m\n",
      "\u001b[48;5;167mH\u001b[48;5;179mell\u001b[48;5;80mo\u001b[0m\n",
      "\u001b[48;5;167mH\u001b[48;5;179mello\u001b[0m\n",
      "\u001b[48;5;167mHello\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m \u001b[48;5;179mW\u001b[48;5;185mo\u001b[48;5;77mr\u001b[48;5;80ml\u001b[48;5;68md\u001b[0m\n",
      "\u001b[48;5;167m \u001b[48;5;179mW\u001b[48;5;185mor\u001b[48;5;80ml\u001b[48;5;68md\u001b[0m\n",
      "\u001b[48;5;167m \u001b[48;5;179mW\u001b[48;5;185mor\u001b[48;5;80mld\u001b[0m\n",
      "\u001b[48;5;167m W\u001b[48;5;185mor\u001b[48;5;80mld\u001b[0m\n",
      "\u001b[48;5;167m W\u001b[48;5;185morld\u001b[0m\n",
      "\u001b[48;5;167m World\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m \u001b[48;5;179mÔøΩ\u001b[48;5;185mÔøΩ\u001b[48;5;77mÔøΩ\u001b[48;5;80mÔøΩ\u001b[0m\n",
      "\u001b[48;5;167m \u001b[48;5;179mÔøΩ\u001b[48;5;185mÔøΩ\u001b[48;5;77mÔøΩ\u001b[0m\n",
      "\u001b[48;5;167m ÔøΩ\u001b[48;5;185mÔøΩ\u001b[48;5;77mÔøΩ\u001b[0m\n",
      "\u001b[48;5;167m ÔøΩ\u001b[48;5;185mÔøΩ\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[15496, 2159, 50169, 233]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Hello World üëã\") # regex ‚Üí ['Hello', ' World', ' üëã']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e216d6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Special Tokens\n",
    "\n",
    "Special tokens correspond to special strings that change/trigger certain behaviors in LLM text generation.\n",
    "\n",
    "- `<|endoftext|>`: used since GPT-2 to delimit different texts in LLM pre-training\n",
    "\n",
    "They are also used in fine-tuned models to enforce roles (e.g., system, user, assistant, tool)\n",
    "\n",
    "- OpenAI (GPT-4): `<|endoftext|>`, `<|im_start|>`, `<|im_end|>`\n",
    "- Meta (LLaMA 3): `<|begin_of_text|>`, `<|start_header_id|>`, `<|end_header_id|>`, `<|eot_id|>`\n",
    "- Mistral (Tekken): `<unk>`, `<s>`, `</s>`, `[INST]`, `[/INST]`, `[AVAILABLE_TOOLS]`, `[/AVAILABLE_TOOLS]`, ...\n",
    "\n",
    "---\n",
    "\n",
    "They are added as a single token to the vocabulary. For example, in GPT-2,\n",
    "\n",
    "vocab. size = 256 (raw bytes) + 50000 (BPE merges) + 1 (special token) = 50257"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40df030c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Issues with Text Tokenization\n",
    "\n",
    "Much of LLMs' weirdness originates from the tokenization step:\n",
    "\n",
    "- Low performance at character-level tasks (spelling, reversal, counting)\n",
    "- Poor performance on non-English languages\n",
    "- Inability to perform simple arithmetic\n",
    "- Abrupt halts when encountering certain characters (e.g., \"<|endoftext|>\")\n",
    "- Trailing whitespace inconsistencies\n",
    "- Undertrained embeddings (e.g., \"SolidGoldMagikarp\")\n",
    "- Arbitrary implementation choices (e.g., regex patterns)\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1fa51c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Byte Latent Transformer (BLT)\n",
    "\n",
    "[paper](https://arxiv.org/abs/2412.09871)\n",
    "\n",
    "Replace tokenization with a trainable layers that operates on sequence of raw bytes: group them (patch) and generated the corresponding embeddings.\n",
    "\n",
    "- BLT (8B) match llama 3 (8B) performance with 50% fewer FLOPs.\n",
    "- BLT more robust to noisy inputs.\n",
    "- BLT have enhanced preformance for char. level task.\n",
    "- BLT scales better (more layers, bigger patches) with same FLOPs budget.\n",
    "\n",
    "*‚Äútokens‚Äù refers to byte-groups drawn from a finite vocabulary determined prior to\n",
    "training as opposed to ‚Äúpatches‚Äù which refer to dynamically grouped sequences without a fixed vocabulary.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3932354",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  2. Patching: from indvidual Bytes to group of Bytes\n",
    "\n",
    "- **Strided Patching Every K Bytes**\n",
    "    - not clever compute allocation.\n",
    "    - the same word being differently.\n",
    "- **Space Patching**\n",
    "    - cannot gracefully handle all languages and domains\n",
    "    - cannot vary the patch size\n",
    "- **Entropy Patching**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be3d381",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Entropy Patching\n",
    "\n",
    "A small char. level LM is used to predict the distribution of the next char.\n",
    "\n",
    "![Figure4.png](https://media.githubusercontent.com/media/S1M0N38/tokenization-notes/refs/heads/main/figures/Figure4.png?token=AFJ2AVQZ4QTG7C7G6RD33CLHPV36C)\n",
    "\n",
    "$$\n",
    "H(x_i) = \\sum_{v \\in \\mathcal{V}} \n",
    "p_e (x_i = v | \\boldsymbol{x}_{\\lt i})\n",
    "\\log p_e (x_i = v | \\boldsymbol{x}_{\\lt i})\n",
    "$$\n",
    "\n",
    "Split at $H(x_t) > \\theta_g$ or  $H(x_t) - H(x_{t - 1}) > \\theta_r$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ab2abc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Figure3.png](https://media.githubusercontent.com/media/S1M0N38/tokenization-notes/refs/heads/main/figures/Figure3.png?token=AFJ2AVQHGJC6NCWM6WTSHDLHPV37Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63164f72",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3 BLT Architecture\n",
    "\n",
    "![Figure1.png](https://media.githubusercontent.com/media/S1M0N38/tokenization-notes/refs/heads/main/figures/Figure1.png?token=AFJ2AVRCCIYS4S34QDYRU6THPV4BM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642a9beb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Local Encoder & Decoder\n",
    "\n",
    "![Figure5.png](https://media.githubusercontent.com/media/S1M0N38/tokenization-notes/refs/heads/main/figures/Figure5.png?token=AFJ2AVXB6JDTS4X4DHODUU3HPV4D2)\n",
    "\n",
    "*Encoder make use of n-gram Embeddings*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a95cd1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 9 Limitation and Future Work\n",
    "\n",
    "- Calculation of scaling law for BLT.\n",
    "- Scale beyond 8b params.\n",
    "- Runtime optimization (FLOPs != wall-clock time).\n",
    "- Learning the patching model end-to-end."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "tokenization-notes",
   "language": "python",
   "name": "tokenization-notes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
